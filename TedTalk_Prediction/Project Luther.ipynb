{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Luther\n",
    "\n",
    "The goal of project luther is to create a linear regression model for supervised machine learning. We also need to incorporate web scrapping into the project to get practice finding what we need.  \n",
    "\n",
    "I will be pulling data from Ted.com.  Specifically, I will be grabbing view count, tags and the published date.  We also will initiate our proxies here, and I will tell why we need them when they are employed below.\n",
    "\n",
    "Packages needed for this portion:\n",
    " * Pandas\n",
    " * Numpy\n",
    " * BeautifulSoup\n",
    " * Requests\n",
    " * Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Proxies need to be changed each run until data is collected! All are from https://free-proxy-list.net/\n",
    "proxies = {\n",
    "    \"http\": 'http://54.153.105.149:80', \n",
    "    \"https\": 'http://54.153.105.149:80'\n",
    "}\n",
    "prox2 = {\n",
    "    \"http\": 'http://108.61.23.81:8080', \n",
    "    \"https\": 'http://108.61.23.81:8080'\n",
    "}\n",
    "prox3 = {\n",
    "    \"http\": 'http://52.179.123.54:3128', \n",
    "    \"https\": 'http://52.179.123.54:3128'\n",
    "}\n",
    "prox4 = {\n",
    "    \"http\": 'http://35.169.210.105:3389', \n",
    "    \"https\": 'http://35.169.210.105:3389'\n",
    "}\n",
    "base = 'https://www.ted.com'\n",
    "start_url = 'https://www.ted.com/talks'\n",
    "\n",
    "response = rq.get(start_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = response.text\n",
    "mainpage = soup(page,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "talks = re.compile('/talks/')\n",
    "views = re.compile(\"Related Tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetVid(mainlist):\n",
    "    vid_str = []\n",
    "    l = len(mainlist)-1\n",
    "    for i in range(0,l,2):\n",
    "        new_str = str(mainlist[i])\n",
    "        vid_str.append(new_str.split('\"')[5])\n",
    "    return vid_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting all cases where we have a /talks url\n",
    "list_1 = mainpage.find_all(href=talks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/talks/tracee_ellis_ross_a_woman_s_fury_holds_lifetimes_of_wisdom',\n",
       " '/talks/dayo_ogunyemi_visions_of_africa_s_future_from_african_filmmakers',\n",
       " '/talks/clemantine_wamariya_war_and_what_comes_after',\n",
       " '/talks/gwynne_shotwell_spacex_s_plan_to_fly_you_across_the_globe_in_30_minutes',\n",
       " '/talks/diane_wolk_rogers_a_parkland_teacher_s_homework_for_us_all',\n",
       " '/talks/zachary_r_wood_why_it_s_worth_listening_to_people_we_disagree_with',\n",
       " '/talks/nancy_rabalais_the_dead_zone_of_the_gulf_of_mexico',\n",
       " '/talks/mark_tyndall_the_harm_reduction_model_of_drug_addiction_treatment',\n",
       " '/talks/hannah_burckstummer_a_printable_flexible_organic_solar_cell',\n",
       " '/talks/yasin_kakande_what_s_missing_in_the_global_debate_over_refugees',\n",
       " '/talks/robin_steinberg_what_if_we_ended_the_injustice_of_bail',\n",
       " '/talks/jaron_lanier_how_we_need_to_remake_the_internet',\n",
       " '/talks/malika_whitley_how_the_arts_help_homeless_youth_heal_and_build',\n",
       " '/talks/lera_boroditsky_how_language_shapes_the_way_we_think',\n",
       " '/talks/jose_andres_how_a_team_of_chefs_fed_puerto_rico_after_hurricane_maria',\n",
       " '/talks/tara_houska_the_standing_rock_resistance_and_our_fight_for_indigenous_rights',\n",
       " '/talks/kasiva_mutua_how_i_use_the_drum_to_tell_my_story',\n",
       " '/talks/danny_hillis_exploring_options_for_solar_geoengineering',\n",
       " '/talks/andrew_dent_to_eliminate_waste_we_need_to_rediscover_thrift',\n",
       " '/talks/drew_philp_my_500_house_in_detroit_and_the_neighbors_who_helped_me_rebuild_it',\n",
       " '/talks/irina_kareva_math_can_help_uncover_cancer_s_secrets',\n",
       " '/talks/raphael_arar_how_we_can_teach_computers_to_make_sense_of_our_emotions',\n",
       " '/talks/christian_picciolini_my_descent_into_america_s_neo_nazi_movement_and_how_i_got_out',\n",
       " '/talks/judith_heumann_our_fight_for_disability_rights_and_why_we_re_not_done_yet',\n",
       " '/talks/leo_igwe_why_i_choose_humanism_over_faith',\n",
       " '/talks/ndidi_nwuneli_the_role_of_faith_and_belief_in_modern_africa',\n",
       " '/talks/erica_stone_academic_research_is_publicly_funded_why_isn_t_it_publicly_available',\n",
       " '/talks/mennat_el_ghalid_how_fungi_recognize_and_infect_plants',\n",
       " '/talks/vikram_sharma_how_quantum_physics_can_make_encryption_stronger',\n",
       " '/talks/matthias_mullenbeck_what_if_we_paid_doctors_to_keep_people_healthy',\n",
       " '/talks/amishi_jha_how_to_tame_your_wandering_mind',\n",
       " '/talks/sauti_sol_the_rhythm_of_afrobeat',\n",
       " '/talks/eve_abrams_the_human_stories_behind_mass_incarceration',\n",
       " '/talks/vittorio_loreto_need_a_new_idea_start_at_the_edge_of_what_is_known',\n",
       " '/talks/soka_moses_for_survivors_of_ebola_the_crisis_isn_t_over',\n",
       " '/talks/bob_stein_a_rite_of_passage_for_late_life']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pulling a list of all video end url links\n",
    "lnk_list = GetVid(list_1)\n",
    "lnk_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a list for all ted talks\n",
    "\n",
    "The ted talks pagination has the url /talks?page=3, therefore we can simplify this to a 'talks?page=' + a list of numbers from 2 to 77 in '77'.  We need to cycle through group of proxies to avoid getting blocked from the site.  We never get permanently 'blocked', but we can get blocked temporarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_lst = [str(i) for i in range(2,78)]\n",
    "mid = '/talks?page='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a list of all the search pages to pull video links from\n",
    "url_list = [base+mid+i for i in page_lst]\n",
    "url_list.insert(0,'https://www.ted.com/talks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function I will call below for each video page\n",
    "def GetVidLst(vurl, prox = None):\n",
    "    resp = rq.get(vurl, prox)\n",
    "    ppage = resp.text\n",
    "    vidpage = soup(ppage,\"lxml\")\n",
    "    vlist = vidpage.find_all(href=talks)\n",
    "    vid_lnk_list = GetVid(vlist)\n",
    "    return vid_lnk_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of all the ted videos\n",
    "fin = []\n",
    "count=0\n",
    "proxy = None\n",
    "for i in url_list:\n",
    "    count+=1\n",
    "    fin +=GetVidLst(i, proxy)\n",
    "    time.sleep(5)\n",
    "    if count == 30 or count == 60 or count == 90 or count == 120:\n",
    "        time.sleep(10)\n",
    "        if count ==30:proxy = proxies\n",
    "        if count ==60: proxy = prox2\n",
    "        if count ==90: proxy = prox3\n",
    "        if count ==120: \n",
    "            proxy = None \n",
    "            count = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ted Talk Dictionary\n",
    "\n",
    "Now that we have our list of all the videos, we are going to create a dictionary with the following format:\n",
    "{'Title':\\['Views', \\['Tags'\\], Date Posted\\]}.  \n",
    "\n",
    "To accomplish this, I first did all the work on a single page, then I will create functions for each step and call them in a for loop over the list 'fin'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changes views string to an integer\n",
    "def view_to_int(viewstring):\n",
    "    viewstring= viewstring.replace('\\n', '').replace(',', '')\n",
    "    return int(viewstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the view count from the site\n",
    "def get_Views(site_text):\n",
    "    views_list = []\n",
    "    viewint=0\n",
    "    views = site_text.find_all(class_=\" d:n f-w:700 f:.9 f:1@xxl c:white \")\n",
    "    str_list2 = str(views).split('<span')[1].split('\\t')\n",
    "    viewint = view_to_int(str_list2[5])\n",
    "    return viewint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets ratings from the HTML\n",
    "def get_ratings(vidpage):\n",
    "    super_comment = webpage.find_all(text = re.compile('\\\"ratings\\\":'))\n",
    "    the_max = max(super_comment, key=len)\n",
    "    super_comment=the_max.split('\\\"ratings\\\":')\n",
    "    super_comment = super_comment[1].split(\"\\\"name\\\":\")\n",
    "    target_ratings = ['Confusing', 'Funny',  'Unconvincing', 'Longwinded', 'Inspiring', 'Fascinating', 'Jaw-dropping']\n",
    "    count = 0\n",
    "    rate_dict = {}\n",
    "    for i in super_comment:\n",
    "        if len(i)>10:\n",
    "            str_list = i.split(\",\")\n",
    "            rate_name = str_list[0].replace(\"\\\"\",'')\n",
    "            rate_count = str_list[1].replace('\\\"count\\\":','').split('}')[0]\n",
    "            if rate_name in target_ratings:\n",
    "                rate_dict[rate_name] = float(rate_count)\n",
    "    return rate_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up site text to get tags\n",
    "def Cleanup(site_text):\n",
    "    super_comment = site_text.find_all(text = re.compile('tag'))\n",
    "    the_max = max(super_comment, key=len)\n",
    "    super_comment=the_max.split('tags')\n",
    "    super_comment = super_comment[1].split(\"\\\"\")\n",
    "    tag=[]\n",
    "    tag =[super_comment[i] for i in range(2,9,2)]\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets published date form HTML\n",
    "def getDate(site_text):\n",
    "    super_comment = str(site_text.find_all(text = re.compile('recorded_at')))\n",
    "    inter = re.findall(re.compile('\"recorded_at\"'), super_comment)\n",
    "    intermed = super_comment.split(inter[0])\n",
    "    intermed = intermed[1].split(':')\n",
    "    date = intermed[1].replace('T00', '').replace('\"', \"\")\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '\"viewed_count\":772886'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-c7208926616b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtag_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtot_views\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_Views\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mratings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ratings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtot_views\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mratings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m40\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m80\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-81-bba9158ce9f1>\u001b[0m in \u001b[0;36mget_ratings\u001b[0;34m(vidpage)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mrate_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mrate_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\\"count\\\":'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mrate_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrate_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrate_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '\"viewed_count\":772886'"
     ]
    }
   ],
   "source": [
    "#Creating a list of all the ted vides\n",
    "data = {}\n",
    "count=0\n",
    "tot_views=[]\n",
    "proxy = prox4\n",
    "for i in fin:\n",
    "    time.sleep(15)\n",
    "    count+=1\n",
    "    curl = base+i\n",
    "    response = rq.get(curl, proxy)\n",
    "    web_page = response.text\n",
    "    webpage = soup(web_page,\"lxml\")\n",
    "    date = getDate(webpage)\n",
    "    tag_list = Cleanup(webpage)\n",
    "    tot_views = get_Views(webpage)\n",
    "    ratings = get_ratings(webpage)\n",
    "    data[i] = [date,tot_views,tag_list,ratings]\n",
    "    if count == 20 or count == 40 or count == 60 or count == 80:\n",
    "        time.sleep(10)\n",
    "        if count ==20:proxy = proxies\n",
    "        if count ==40: proxy = prox2\n",
    "        if count ==60: proxy = prox4\n",
    "        if count ==80: \n",
    "            proxy = prox3\n",
    "            count = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing a pickle file to save data\n",
    "\n",
    "with open('my_data.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(data, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Pandas\n",
    "\n",
    "Now that we have our dictionary of values, we are changing it to a pandas dataframe for easier manipulation.\n",
    "\n",
    "There will then be some prelimnary exploring to see if it is formatted correctly or if more manipulation is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading a pickle file reading to pick up where i left off in case something fails or i start over\n",
    "with open(\"my_data.pkl\", 'rb') as picklefile: \n",
    "    my_old_data = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Data\n",
    "\n",
    "There is data from a kaggle that incorporates similar things above, and I will use it to enrich my data.  I will also substitute using it now because I am still troubleshooting pulling data from ted.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading a pickle file from other notebook to get all the url lists I need to update data\n",
    "with open(\"url_list.pkl\", 'rb') as picklef: \n",
    "    kag_url = pickle.load(picklef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a dictionary of all the info I need from the videos in the kag_url list\n",
    "data = {}\n",
    "count=0\n",
    "tot_views=[]\n",
    "proxy = prox2\n",
    "for i in kag_url:\n",
    "    time.sleep(30)\n",
    "    count+=1\n",
    "    response = rq.get(i, proxy)\n",
    "    web_page = response.text\n",
    "    webpage = soup(web_page,\"lxml\")\n",
    "    date = getDate(webpage)\n",
    "    tag_list = Cleanup(webpage)\n",
    "    tot_views = get_Views(webpage)\n",
    "    ratings = get_ratings(webpage)\n",
    "    data[i] = [date,tot_views,tag_list,ratings]\n",
    "    if count == 20 or count == 40 or count == 60 or count == 80:\n",
    "        time.sleep(10)\n",
    "        if count ==20:proxy = proxies\n",
    "        if count ==40: proxy = prox2\n",
    "        if count ==60: proxy = prox4\n",
    "        if count ==80: \n",
    "            proxy = prox3\n",
    "            count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving for future use, as the above code runs for at least 2 hours\n",
    "with open('test_data.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(data, picklefile)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
